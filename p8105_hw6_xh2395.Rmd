---
title: "p8105_hw6_xh2395"
author: "Xin  He"
date: "11/24/2019"
output: github_document
---

```{r}
library(tidyverse)
library(modelr)
library(mgcv)
```

## Problem 1

**Load the data**

```{r}
birthweight_df =
  read_csv("./data/birthweight.csv") 
```

**Clean the data**

```{r}
bw_clean = 
  birthweight_df %>% 
  mutate(
  babysex=as.factor(babysex),
  babysex=recode(babysex,"1"="male", "2"="female"),
  frace=as.factor(frace),
  frace=recode(frace,"1"="white", "2"="black","3"="asian", "4"="puerto rican", "8"="other","9"="unknown"),
  malform=as.factor(malform),
  malform=recode(malform,"1"="present", "0"="absent"),
  mrace=as.factor(mrace),
  mrace=recode(mrace, "1"="white", "2"="black","3"="asian", "4"="puerto rican", "8"="other"))
```

**Check missing data**

```{r}
anyNA(bw_clean)
```

There is no missing data in this dataset.

### Propose a regression model with all predictors

```{r}
model_1 = lm(bwt ~ ., data = bw_clean)

summary(model_1)
```

There are 3 variables not defined because of singularities: pnumlbw, pnumsga, wtgain.

### Exclude pnumlbw, pnumsga, wtgain

```{r}
model_2 = update(model_1, . ~ . -pnumlbw -pnumsga -wtgain)

summary(model_2)
```

There are some non-significant variables. Since all frace-related variables are non-significant, we can exclude frace.

### Exclude frace

```{r}
model_3 = update(model_2, . ~ . -frace)

summary(model_3)
```

There are still some non-significant variables. We can exclude these with big p-values (>0.5): malform, momage, ppbmi.

### Exclude malform, momage, ppbmi

```{r}
model_4 = update(model_3, . ~ . -malform -momage -ppbmi)

summary(model_4)
```

The p-value of menarche is still bigger than 0.1, we can exclude it.

### Exclude menarche

```{r}
model_5 = update(model_4, . ~ . -menarche)

summary(model_5)
```

Now, the variable mrace has the biggest p-value. Let's try to exlude it.

### Exclude mrace

```{r}
model_6 = update(model_5, . ~ . -mrace)

summary(model_6)
```

Now, all left variables are with a significant p-value.

**The final model:**

lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + parity + ppwt + smoken, data = bw_clean)

**Tidy and table the final model**

```{r}
model_6 %>% 
  broom::tidy() %>% 
  knitr::kable()
```

**Plot of model residuals against fitted values**

```{r}
bw_clean %>% 
modelr::add_residuals(model_6) %>% 
modelr::add_predictions(model_6) %>% 
  ggplot(aes(x = pred, y = resid, color = bwt)) + 
  geom_point(alpha = 0.5)+
  labs(
    title = 'Model residuals against fitted values',
    x = 'Fitted values',
    y = 'Residuals'
  )
```

**Two other models**

```{r}
other_1 = lm(bwt ~ blength + gaweeks, data = bw_clean)
broom::tidy(other_1)

other_2 = lm(bwt ~ bhead * blength + blength * babysex + bhead * babysex, data = bw_clean)
broom::tidy(other_2)
```

**Comparison in terms of the cross-validated prediction error**

```{r}
cv_df = 
  crossv_mc(bw_clean, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(model_6_mod  = map(train, ~model_6),
         other_1_mod  = map(train, ~other_1),
         other_2_mod  = map(train, ~other_2)) %>% 
  mutate(rmse_model_6 = map2_dbl(model_6_mod, test, ~rmse(model = .x, data = .y)),
         rmse_other_1 = map2_dbl(other_1_mod, test, ~rmse(model = .x, data = .y)),
         rmse_other_2 = map2_dbl(other_2_mod, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
    geom_violin()
```

According to the plot, my own model has the lowest root mean square error and thus seems to be the model that predicts birthweight best.


## Problem 2

**Download the data**

```{r}
set.seed(10)

weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

### Get 5000 log(beta0 * beta1)

```{r}
beta0_beta1 = 
  weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(-std.error, -statistic, -p.value) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate) %>% 
  janitor::clean_names() %>% 
  mutate(log_b0b1 = log(intercept * tmin))
```

**Plot the distribution of log(beta0 * beta1)**

```{r}
beta0_beta1 %>% 
  ggplot(aes(x = log_b0b1)) +
  geom_density() + 
  labs(
    title = 'Distribution of log_beta0*beta1 estimates'
  )
```

The log(beta0 * beta1) is a symmtrical bell shaped curve that follows a normal distribution. This is most likely because of the large bootstrap sample.

**The 95% confidence interval for log(beta0 * beta1)**

```{r}
beta0_beta1 %>% 
  pull(log_b0b1) %>% 
  quantile(., probs = c(0.025, 0.975), na.rm = TRUE)
```

The 95% confidence interval of log(beta0 * beta1) is (1.964572, 1.964572).

### Get 5000 R2

```{r}
R2 = 
  weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  janitor::clean_names()
```

**Plot the distribution of R2**

```{r}
R2 %>% 
  ggplot(aes(x = r_squared)) +
  geom_density() + 
  labs(
    title = 'Distribution of R2 estimates'
  )
```

The plot of r2 estimates is bell shaped and follows a normal distribution. This is most likely because of the large bootstrap sample.

**The 95% confidence interval for R2**

```{r}
R2 %>% 
  pull(r_squared) %>% 
  quantile(., probs = c(0.025, 0.975), na.rm = TRUE)
```

The 95% confidence interval of R2 is (0.8938239, 0.9268347).












